<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CAI Semantic Equivalence Benchmark | Compression-Aware Intelligence</title>
  <meta name="description" content="CAI semantic equivalence benchmark: 300 pairs of meaning-preserving prompts that expose internal contradictions and compression strain in large language models under semantic paraphrase." />
  <link rel="canonical" href="https://compressionawareintelligence.com/dataset.html" />

  <!-- Favicons -->
  <link rel="icon" href="favicon.ico" sizes="any">
  <link rel="icon" href="favicon.png" type="image/png">
  <link rel="apple-touch-icon" href="apple-touch-icon.png">

  <!-- Open Graph / Twitter -->
  <meta property="og:title" content="CAI Semantic Equivalence Benchmark" />
  <meta property="og:description" content="300 semantically equivalent prompt pairs that turn hallucinations and drift into a measurable internal coherence signal for frontier models." />
  <meta property="og:image" content="assets/cai-card.png" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://compressionawareintelligence.com/dataset.html" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CAI Semantic Equivalence Benchmark">
  <meta name="twitter:description" content="A plug in benchmark for internal coherence under semantic equivalence, across factual, ethical, planning, and CAI specific prompts.">
  <meta name="twitter:image" content="assets/cai-card.png">

  <!-- Styles and scripts -->
  <link rel="stylesheet" href="styles.css" />
  <script defer src="script.js"></script>
</head>
<body>
<header class="site-header" role="banner">
  <a class="brand" href="index.html" aria-label="Compression-Aware Intelligence home">
    <span class="logo" aria-hidden="true"></span>
    <span>CAI</span>
  </a>
  <nav class="nav" aria-label="Primary">
    <a href="index.html" data-nav>Home</a>
    <a href="proof.html" data-nav>Proof</a>
    <a href="dataset.html" data-nav>Dataset</a>
    <a href="coherencefield.html" data-nav>The Coherence Field</a>
    <a href="principles.html" data-nav>Principles</a>
    <a href="applications.html" data-nav>Applications</a>
    <a href="contradictions.html" data-nav>Contradictions</a>
    <a href="implementations.html" data-nav>Implementations</a>
    <a href="https://semantic-stability-lab-clinical-59orjfx58.vercel.app" target="_blank" rel="noopener">Stability Lab</a>
    <a href="glossary.html" data-nav>Glossary</a>
    <a href="faq.html" data-nav>FAQ</a>
    <a href="about.html" data-nav>About</a>
    <a href="contact.html" data-nav>Contact</a>
  </nav>
</header>

<main class="wrap" id="content">
  <!-- HERO -->
  <section class="hero card" aria-labelledby="hero-title">
    <p class="pill">Dataset · November 2025</p>
    <h1 id="hero-title">CAI semantic equivalence benchmark</h1>
    <p class="lede">
      <strong>can your model keep a single coherent answer inside one semantic class when you paraphrase the prompt?</strong>
    </p>
    <p>
      the benchmark contains 300 pairs of meaning-preserving prompts across factual questions, everyday reasoning, math,
      hypotheticals, ethics, social and planning questions, creative writing, summarization, paraphrase, and CAI specific meta prompts.
      Each pair is designed so that a competent model should give the same underlying answer, even if wording shifts.
    </p>

    <div class="key-callout card">
      <h2 class="small-heading">why this dataset exists</h2>
      <ul class="bullets">
        <li> collapse degrees of freedom because semantic equivalence removes style and phrasing as excuses, so disagreement inside a pair is a real internal conflict, not a preference.</li>
        <li> turn hallucinations into a coherence signal because contradictions and drifts under paraphrase show up as <strong>CAI strain</strong> instead of scattered anecdotes.</li>
        <li> plug in to any agenda and use the pairs as a drop in testbed for truthfulness training, belief models, logit probes, or alignment dashboards.</li>
      </ul>
    </div>

    <p class="muted">
      on this benchmark, <strong>gpt-4o contradicts itself or meaningfully drifts on roughly 36 percent of semantic equivalence pairs</strong>,
      even at temperature 0.
    </p>
    <div class="cta-row">
      <a class="btn" href="https://github.com/michelejoseph1/cai-semantic-equivalence-benchmark" target="_blank" rel="noopener">
        view dataset and code on GitHub
      </a>
      <a class="btn ghost" href="dataset.csv">Download dataset.csv</a>
    </div>
    <p class="muted small">
      below are scores and examples for gpt-4o. You can rerun everything on your own models with one script.
    </p>
  </section>

  <!-- MODEL SCORES -->
  <section class="card" aria-labelledby="scores-title">
    <h2 id="scores-title">Current model scores</h2>
    <p>
      the evaluation script calls a chat model on each prompt in the pair, then computes a
      <strong>CAI strain v2</strong> score based on agreement, numeric consistency, and direct contradictions.
      a score of 0 means fully consistent answers inside each pair. A score of 1 means maximally contradictory or divergent under paraphrase.
    </p>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Pairs</th>
            <th>CAI strain v2 (0 - 1)</th>
            <th>Simple string mismatch rate</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>gpt-4o</code></td>
            <td>300</td>
            <td>0.3642</td>
            <td>0.9900</td>
            <td>
              Run using <code>evaluate_openai.py</code> on November 15, 2025 with temperature 0.
              String mismatch rate is the naive baseline where any non identical answer after lowercasing counts as different.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <p class="muted small">
      The gap between the simple string mismatch rate and CAI strain shows that models can vary wording almost always,
      but only some of those variations reflect genuine changes in what the model is saying or recommending.
      CAI strain tries to focus on the latter.
    </p>
    <p class="muted small">
      Reproduce the numbers with your own key:
    </p>
    <pre class="code"><code>git clone https://github.com/michelejoseph1/cai-semantic-equivalence-benchmark.git
cd cai-semantic-equivalence-benchmark
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY="YOUR_KEY_HERE"
python evaluate_openai.py --model gpt-4o --max_pairs 300</code></pre>
    <p class="muted small">
      This produces <code>results_gpt-4o.csv</code> with full prompts, answers, and per pair strain scores, and appends a row to <code>scores.csv</code>.
    </p>
  </section>

  <!-- DISTRIBUTION GRAPH -->
  <section class="card" aria-labelledby="dist-title">
    <h2 id="dist-title">Strain distribution for gpt-4o</h2>
    <p>
      CAI strain is a graded signal, not an all or nothing flag. The figure below shows the distribution of per pair
      CAI strain v2 scores for gpt-4o on the full dataset. The right side mass highlights a long tail of internal conflicts
      that only show up once you hold meaning fixed and move the wording around.
    </p>

    <figure class="chart" style="max-width: 600px; margin: 0 auto; padding: 1rem; border: 1px solid #eee; border-radius: 12px;">
      <img src="strain_distribution.png"
           alt="Histogram of CAI strain v2 scores for gpt-4o on 300 semantic equivalence pairs"
           style="width: 100%; height: auto; display: block;" />
      <figcaption class="muted small" style="text-align:center; margin-top: 0.5rem;">
        Histogram of CAI strain v2 scores for gpt-4o across the 300 semantic equivalence pairs.
      </figcaption>
    </figure>
  </section>

  <!-- DOWNLOADS -->
  <section class="card" aria-labelledby="files-title">
    <h2 id="files-title">files</h2>
    <ul class="bullets">
      <li><a href="dataset.csv">dataset.csv</a> – 300 rows with <code>pair_id,prompt_A,prompt_B</code>.</li>
      <li><a href="dataset.json">dataset.json</a> – the same 300 pairs as a JSON list.</li>
      <li><a href="results_gpt-4o.csv">results_gpt-4o.csv</a> – gpt-4o outputs and per pair CAI strain v2 scores.</li>
      <li><a href="scores.csv">scores.csv</a> – summary line per run (timestamp, model, num_pairs, avg_strain_v2).</li>
      <li><a href="https://github.com/michelejoseph1/cai-semantic-equivalence-benchmark" target="_blank" rel="noopener">
        GitHub repo with evaluation script</a> – <code>evaluate_openai.py</code> and scoring logic.
      </li>
    </ul>
  </section>

  <!-- EXAMPLES -->
  <section class="card" aria-labelledby="examples-title">
    <h2 id="examples-title">example pairs and gpt-4o behavior</h2>
    <p>
      here are real examples showing low strain (stable behavior), medium strain (semantic drift),
      and high strain (direct contradiction) under paraphrase.
    </p>

    <!-- LOW STRAIN EXAMPLE -->
    <article class="example">
      <h3>Example 1 – low strain (0.00)</h3>
      <p class="muted small">
        Factual question where gpt-4o gives effectively identical answers to both paraphrases.
      </p>
      <div class="grid two">
        <div>
          <h4>Pair 1 prompts</h4>
          <p><strong>prompt_A</strong>: Who wrote Pride and Prejudice?</p>
          <p><strong>prompt_B</strong>: Which author is responsible for the novel Pride and Prejudice?</p>
        </div>
        <div>
          <h4>gpt-4o answers</h4>
          <p><strong>answer_A</strong>:
            "Pride and Prejudice" was written by Jane Austen.
          </p>
          <p><strong>answer_B</strong>:
            The novel "Pride and Prejudice" was written by Jane Austen.
          </p>
          <p><strong>CAI strain v2</strong>: 0.00</p>
        </div>
      </div>
      <p class="muted small">
        This is the desired behavior. The semantic equivalence class collapses to a single stable internal answer.
      </p>
    </article>

    <!-- MEDIUM STRAIN EXAMPLE -->
    <article class="example">
      <h3>Example 2 – medium strain (0.47)</h3>
      <p class="muted small">
        Everyday reasoning question where gpt-4o gives partially aligned answers, but the paraphrase
        pulls the model into a different emphasis and a different implied recommendation.
        This is the kind of subtle drift CAI is meant to surface.
      </p>

      <div class="grid two">
        <div>
          <h4>Pair 143 prompts</h4>
          <p><strong>prompt_A</strong>: You are studying for an exam in 2 weeks. How should you structure your study sessions?</p>
          <p><strong>prompt_B</strong>: What is an effective two week study plan before an important test?</p>
        </div>

        <div>
          <h4>gpt-4o answers</h4>
          <p><strong>answer_A</strong>:  
            Create a schedule that spreads out material over the full 14 days, mixing review, practice problems,
            and spaced repetition. Aim for shorter daily sessions to keep engagement high.
          </p>

          <p><strong>answer_B</strong>:  
            Focus heavily on practice tests during the first week to diagnose weaknesses, then dedicate the second week
            to relearning weak areas in longer multi hour blocks with minimal breaks.
          </p>

          <p><strong>CAI strain v2</strong>: 0.47</p>
        </div>
      </div>

      <p class="muted small">
        both answers are reasonable, but they imply different study philosophies. One emphasizes distributed practice
        and short daily sessions while the other recommends front loading diagnostics and long blocks in week two.
        this is neither fully stable nor fully contradictory, which is exactly the kind of semantic drift that CAI strain tries to quantify.
      </p>
    </article>

    <!-- HIGH STRAIN EXAMPLE -->
    <article class="example">
      <h3>Example 3 – high strain (1.00)</h3>
      <p class="muted small">
        Factual question where the model changes its mind under a paraphrase about which country has the largest population.
      </p>
      <div class="grid two">
        <div>
          <h4>Pair 17 prompts</h4>
          <p><strong>prompt_A</strong>: What country has the most people?</p>
          <p><strong>prompt_B</strong>: Which nation has the largest population?</p>
        </div>
        <div>
          <h4>gpt-4o answers</h4>
          <p><strong>answer_A</strong>:
            As of the latest data, China has the largest population, although India is very close in population size and is projected to surpass China in the near future.
          </p>
          <p><strong>answer_B</strong>:
            As of the most recent data, India is the nation with the largest population, having surpassed China. This makes India the most populous country in the world.
          </p>
          <p><strong>CAI strain v2</strong>: 1.00</p>
        </div>
      </div>
      <p class="muted small">
        Both prompts ask the same question. The first answer makes a confused, outdated claim about China,
        while the second correctly states that India is now most populous.
        Under CAI this is a clean example of a hidden internal conflict that only appears under semantic perturbation.
      </p>
    </article>
  </section>

  <!-- ALIGNMENT CONTEXT -->
  <section class="card" aria-labelledby="align-title">
    <h2 id="align-title">Why this matters for alignment and reliability work</h2>
    <ul class="bullets">
      <li> belief stability signal because contradictions under paraphrase flag cases where the model does not behave as if it has a single coherent answer for a fixed question.</li>
      <li> cheap frontier stress test because no finetuning or special access needed. Just API calls against a fixed public benchmark with transparent scoring and a single average strain number.</li>
      <li> ready made internal coherence testbed where you can plug these pairs into logit lens analyses, belief graph models, interpretability tools, or safety training schemes as a simple base layer for internal coherence checks.</li>
    </ul>
  </section>

  <!-- HOW TO USE -->
  <section class="card" aria-labelledby="usage-title">
    <h2 id="usage-title">how to use this benchmark</h2>
    <ol class="steps">
      <li>run your model on <code>dataset.csv</code> using the provided script or your own evaluation pipeline.</li>
      <li>compute CAI strain scores per pair. You can start with the simple baseline in the repo or plug in a stronger judge model.</li>
      <li>inspect high strain cases first. These are where contradictions, temporal drift, and compression failures usually live.</li>
    </ol>
    <p class="muted small">
      if you run your own models and are willing to share scores or interesting failure cases,
      feel free to open an issue or pull request on the GitHub repo.
    </p>
  </section>
</main>

<footer class="site-footer" role="contentinfo">
  <small>
    © <span id="year"></span> Compression-Aware Intelligence.
    See also: <a href="https://www.contradictionengineering.com" target="_blank" rel="noopener">Contradiction Engineering</a>
  </small>
</footer>

<script>
  (function () {
    var y = document.getElementById('year');
    if (y) y.textContent = new Date().getFullYear();
  })();
</script>
</body>
</html>
