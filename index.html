<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Compression-Aware Intelligence (CAI) · Reliability layer for AI systems</title>
  <meta name="description" content="Compression-Aware Intelligence Compression-Aware Intelligence (CAI) links hallucination to representational conflict under semantic-equivalence rephrasing, and defines compression strain as the measurable signal of that conflict. CAI is the reliability layer for AI systems. It tests stability across semantically equivalent prompts to detect internal model contradictions and compression failures. According to CAI, compression loss → contradiction → compression strain → instability" />
  <link rel="canonical" href="https://compressionawareintelligence.com/" />

  <!-- Favicons -->
  <link rel="icon" href="favicon.ico" sizes="any">
  <link rel="icon" href="favicon.png" type="image/png">
  <link rel="apple-touch-icon" href="apple-touch-icon.png">

  <!-- Open Graph / Twitter -->
  <meta property="og:title" content="Compression-Aware Intelligence (CAI)" />
  <meta property="og:description" content="CAI measures stability across semantically equivalent prompts to surface contradictions and compression failures before they hit production." />
  <meta property="og:image" content="assets/cai-card.png" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://compressionawareintelligence.com/" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Compression-Aware Intelligence (CAI)">
  <meta name="twitter:description" content="Reliability layer for AI systems: test semantic stability, find contradictions, reduce hallucinations.">
  <meta name="twitter:image" content="assets/cai-card.png">

  <!-- Styles and scripts -->
  <link rel="stylesheet" href="styles.css" />
  <script defer src="script.js"></script>

  <!-- Structured data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Compression-Aware Intelligence",
    "url": "https://compressionawareintelligence.com/",
    "logo": "https://compressionawareintelligence.com/assets/cai-logo.png",
    "sameAs": [
      "https://github.com/michelejoseph1",
      "https://www.contradictionengineering.com"
    ]
  }
  </script>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "SoftwareApplication",
    "name": "CAI Tooling",
    "applicationCategory": "AI reliability, evaluation, governance",
    "operatingSystem": "Cross-platform",
    "offers": {"@type": "Offer", "price": "0"},
    "url": "https://compressionawareintelligence.com/implementations.html"
  }
  </script>
</head>
<body>
<header class="site-header" role="banner">
  <a class="brand" href="index.html" aria-label="Compression-Aware Intelligence home">
    <span class="logo" aria-hidden="true"></span>
    <span>CAI</span>
  </a>
  <nav class="nav" aria-label="Primary">
    <a href="index.html" data-nav>Home</a>
    <a href="proof.html" data-nav>Proof</a>
    <a href="dataset.html" data-nav>Dataset</a>
    <a href="coherencefield.html" data-nav>The Coherence Field</a>
    <a href="principles.html" data-nav>Principles</a>
    <a href="applications.html" data-nav>Applications</a>
    <a href="contradictions.html" data-nav>Contradictions</a>
    <a href="implementations.html" data-nav>Implementations</a>
    <!-- Public demo link -->
    <a href="https://semantic-stability-lab-clinical-59orjfx58.vercel.app" target="_blank" rel="noopener">Stability Lab</a>
    <a href="glossary.html" data-nav>Glossary</a>
    <a href="faq.html" data-nav>FAQ</a>
    <a href="contact.html" data-nav>Contact</a>
  </nav>
</header>

<main class="wrap" id="content">
  <!-- HERO -->
  <section class="hero card" aria-labelledby="hero-title">
    <p class="pill">Updated November 2025</p>
    <h1 id="hero-title">Compression-Aware Intelligence</h1>
    <p class="lede">
      CAI is about one thing: test stability across semantically equivalent but differently phrased prompts to detect internal model contradictions and compression failures.
    </p>
    <div class="cta-row">
      <a class="btn" href="proof.html">Start with the one page proof</a>
      <a class="btn ghost" href="https://semantic-stability-lab-clinical-59orjfx58.vercel.app" target="_blank" rel="noopener">
        Run the 60 second stability demo
      </a>
    </div>
    <p class="muted">
      Every system compresses. If you can see the compression, you can predict where the answer will drift, route before failure, and ship safer systems.
    </p>
  </section>

  <!-- VALUE FOR AI ENGINEERS -->
  <section class="card" aria-labelledby="eng-title">
    <h2 id="eng-title">For AI engineers</h2>
    <div class="grid two">
      <div>
        <ul class="checklist">
          <li><strong>Semantic stability scans</strong>: measure output consistency across meaning-preserving rewrites.</li>
          <li><strong>MirrorNet</strong>: reflect compression choices to user and model at inference time.</li>
          <li><strong>Hallucinet</strong>: estimate risk tied to hidden compression before it hits prod.</li>
          <li><strong>CAC tests</strong>: cost aware checks for evals and QA that surface failure modes early.</li>
          <li><strong>Contradiction maps</strong>: find, score, and fix conflicts worsened by compression.</li>
          <li><strong>Provenance hooks</strong>: attach sources to claims to enable audit and rollback.</li>
        </ul>
      </div>
      <div>
        <h3>Drop in outcomes</h3>
        <ul class="bullets">
          <li>Fewer hallucinations and clearer deltas between prompts.</li>
          <li>Readable risk signals for product, legal, and safety teams.</li>
          <li>Faster incident triage with traceable compression sites.</li>
          <li>Better acceptance criteria for safety gates.</li>
        </ul>
        <p class="muted small">
          We do not ask if the answer is correct. We ask if it is stable.
        </p>
      </div>
    </div>
  </section>

  <!-- QUICK START -->
  <section class="card" aria-labelledby="quick-title">
    <h2 id="quick-title">Quick start</h2>
    <p class="muted">60 seconds to run a stability scan on your docs.</p>
    <pre class="code"><code>git clone https://github.com/michelejoseph1/knowledge_landscape_stability_kit.git
cd knowledge_landscape_stability_kit
pip install -r requirements.txt
python knowledge_landscape_scan.py --docs ./sample_docs --out ./scan_out
open ./scan_out/stability_scan.html</code></pre>
    <div class="cta-row">
      <a class="btn" href="implementations.html">All implementations</a>
      <a class="btn ghost" href="applications.html">See applications</a>
    </div>
  </section>

  <!-- CONCEPT SNAPSHOT -->
  <section class="card" aria-labelledby="concept-title">
    <h2 id="concept-title">Core concepts at a glance</h2>
    <ul class="cols">
      <li><strong>CFI</strong>: coherence under small semantic perturbations.</li>
      <li><strong>Compression strain</strong>: how loss accumulates into failure modes.</li>
      <li><strong>Compression sites</strong>: places where format, time, or abstraction force loss.</li>
      <li><strong>Loss scoring</strong>: quantify how much and what kind of meaning is dropped.</li>
    </ul>
    <p class="muted">Stability across equivalence classes is the early warning signal.</p>
  </section>

  <!-- PROOF + WHY IT MATTERS -->
  <section class="grid two">
    <div class="card">
      <h2>Why it matters</h2>
      <ul class="bullets">
        <li>Turn hidden compression into explicit, auditable choices.</li>
        <li>Make contradictions visible before they become incidents.</li>
        <li>Give stakeholders a shared language for risk.</li>
      </ul>
    </div>
    <div class="card">
      <h2>Proof sketch</h2>
      <p>
        Assume every system compresses. Unseen compression implies unbounded error under distributional shift. Reliable intelligence must expose, measure, and reason about its compression. Compression-Aware Intelligence (CAI) is all about testing stability across semantically equivalent but differently phrased prompts as a way to detect internal model contradictions/compression failures. Full derivation in <a href="proof.html">Proof</a>. Empirical demos in <a href="implementations.html">Implementations</a> and <a href="applications.html">Applications</a>.
      </p>
    </div>
  </section>

  <!-- CALL TO ACTION -->
  <section class="card cta-wide" aria-labelledby="cta-title">
    <h2 id="cta-title">Add CAI to your stack</h2>
    <ol class="steps">
      <li>Run a semantic stability scan on one product surface.</li>
      <li>Wire MirrorNet flags into your UI and logs.</li>
      <li>Adopt CAC tests in CI for high risk prompts.</li>
    </ol>
    <div class="cta-row">
      <a class="btn" href="implementations.html">Get adapters and SDKs</a>
      <a class="btn ghost" href="https://semantic-stability-lab-clinical-59orjfx58.vercel.app" target="_blank" rel="noopener">Open Stability Lab</a>
    </div>
  </section>
</main>

<footer class="site-footer" role="contentinfo">
  <small>
    © <span id="year"></span> Compression-Aware Intelligence.
    See also: <a href="https://www.contradictionengineering.com" target="_blank" rel="noopener">Contradiction Engineering</a>
  </small>
</footer>

<script>
  (function () {
    var y = document.getElementById('year');
    if (y) y.textContent = new Date().getFullYear();
  })();
</script>
</body>
</html>
